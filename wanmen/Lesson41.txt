第四十一次课	RNN
	1.HMM和全连接神经网络训练复杂度过高,参数数量巨大
	  需要解决时间序列依赖性、输入输出不定长等问题
	  不适应权重之间的具有联系
	2.h(t)表示第t个隐藏层,则有
		h(t) = f(U*x(t)+W*h(t-1)+b) = f(U,x,W,h,b)
	  表示当前隐藏层不仅依赖于输入,还依赖于过往的记录
	  *函数f常使用sigmoid或者tanh
	    o(t) = V*h(t) + c
	    y_pred = g(o(t))
	  *函数g可使用softmax等,以实际情况而定
	3.word2vec
	  将字表示为向量,作为输入进行学习
	4.双向递归神经网络
	5.同样存在梯度弥散