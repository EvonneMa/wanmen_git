第十四次课：SVM
	1.核心思想:最大间隔化,不受噪声干扰
	2.SVM是什么？SVM要做什么？
	  定义超平面(以二分类为例)
		w.T*x+b = 0
	  w为法向量,决定超平面方向;x为样本,其中距离超平面最近的几个叫做支持向量
	  假设样本被正确划分,则有
	  	w.T*x+b = 1
	  	w.T*x+b = -1
	  两条直线作为各自的边界,这两条直线的距离称为间隔(margin):
	  	r = 2/||w||
	  显然,间隔越大,对新样本的分类越不易出错――max 2/||w||(损失函数)
	  SVM就是用来解决这样一种凸二次规划问题
	   		min 1/2*||w||^2		――损失函数
	   	s.t.yi(w.T*xi+b)>=1
	   		(xi,yi)来自于样本空间D
	  该问题有最优解且恒为全局最优  
	3.凸函数――――SVM可行的理论基础
		在集合S上,任意两点的连线都位于两点间曲线的同一侧
		常见的有
			仿射函数		w.T*x+b
			二次函数		x.T*A*x+b*x+c		A为半正定矩阵
			最小平方差函数	(||y-A*x||_2)^2		A.T*A总是半正定矩阵
			max函数			max(xi)
	  目标函数、不等式约束均为凸函数,等式约束为仿射函数的优化问题被称为凸优化问题
	  凸优化问题恒有全局最优解
	4.广义拉格朗日乘子法―――――将SVM从约束优化转变为无约束优化,此时还不能对x求导
	  *要理解并能够推导L及各个约束的表达式
		等式约束(g(x) = 0)
			L = f(x) + λ*g(x)	[f(x)和g(x)的导数在最优点x*处共线,正交于约束曲面]
		 => L'(x) = 0
		  	L'(λ) = 0
		不等式约束(g(x) <= 0)
		    L = f(x) + λ*g(x)
		 => L'(x) = 0
		 	Σλ*g(x) = 0	(添加松弛因子变不等式为等式,按等式约束求导后可推导出此约束)
		 	*g(x)<0时约束不起作用,λ=0
			 g(x)=0时约束起作用,λ>0
		 	λ >= 0 (-f'(x)必须落在g'(x)构成的超角锥当中,又由于共面,故系数必须>=0)
		 	*若f(x)的极值落在可行域内,则λ=0即可
		 	 若f(x)的极值不在可行域内,由于g'(x)恒背离可行域,f'(x)恒指向可行域
		 	 	考虑到二者共线(高维情况同理),故只能λ>0
		 	g(x) <= 0 
		 	*以上也称为KKT条件(https://zhuanlan.zhihu.com/p/26514613)
	5.对偶问题――――让L先对x求极值,使得SVM有理由对x求导
		p* = min   max   L(x,μ,λ) ――――主问题
			  x    μ,λ
	<==>
		d* = max   min L(x,μ,λ)   ――――对偶问题  
			 μ,λ    x
		一般地,d* <= p*,称为弱对偶;如果满足KKT条件,则称为强对偶,即d*=p*
		SVM的对偶问题如下式
			max W(λ) = Σλ - 1/2*ΣΣ(yi*yj*λi*λj*xi.T*xj)
		s.t. = KKT约束
	6.支持向量――――让SVM知道需要哪些样本
		由第4点可知,只有g(x)=0的样本才能起到约束作用,这些样本被称为支持向量
	7.核函数――――建立空间映射,使得问题在新空间中是线性的,SVM就能干活啦~~
		假设存在映射g(x),则SVM对偶问题即为
			max W(λ) = Σλ - 1/2*ΣΣ(yi*yj*λi*λj*g(xi).T*g(xj))
		s.t. = KKT约束
		定义核函数Kernel――K(xi,xj) = g(xi).T*g(xj)
		显然,我们不需要知道g(x)的具体表达式,只关心g(xi).T*g(xj)的表达式即可
		这样就避免了构造又难又复杂的映射,转而构造核函数
		常见核函数:
			线性核函数
			多项式
			高斯――――K = exp(-γ(xi-xj)^2)
				高斯核函数中的参数γ同样与拟合程度有关
					γ越大―――>样本之间再细微的差距也会被过分放大而显著――――>过拟合
					γ越小―――>样本之间再大的差距也会被过分缩小而忽略――――>欠拟合
			拉普拉斯
			sigmoid
	8.正则化与软间隔――――让SVM能够容忍一定的出错率、对抗过拟合
		松弛变量(slack variable)ξ
			"越界"样本距离己方分界线的距离,允许ξ>0
		正则化强度参数C――||w||/C
			C越小,w越接近于0,模型普适性强但容易欠拟合;反之过拟合
			*对比一下线性回归罚函数的表达式~~~~
		此时的支持向量为所有g(x)<=0(即越界)的样本
	9.SVR――――SVM用于回归
		 将落在margin以外的样本加入到损失函数中,此时的支持向量即为这些g(x)>1的样本
	10.多类分类
		OVO OVR
	*综上所述：
		1.支持向量是那些对模型建立和优化有贡献的样本,不同情况有不同的标准
		2.所有的有约束的优化问题都可以使用广义拉格朗日乘子法去掉约束,得到KKT条件
		3.由于不等式的存在,无法消去λ只留下x,但可以消去x留下λ,从而成为一元函数
		4.为了使得先消x和先消λ两种方法得到的最优解是一样的,我们要求求解时必须遵守KKT条件
		5.优点――核函数把问题变线性
				支持向量使样本量大大减小
		  缺点――不适合大数据
	11.python代码
		from sklearn.svc import SVC,SVR
		clf_svc = SVC()
		clf_svr = SVR()
		#主要参数――C,kernel,decision_function_shape等
		clf.fit()
		clf.predict()