第十二次课：决策树与随机森林
	1.决策树将是一棵二叉树(尽可能是),通过不断做判断来决定样本属于那一类
	2.属性选择度量(优先选择分类结果最纯洁的类别)
	    叶节点(j是目标属性具有的全部类别数目)
			Entropy = Σ-pj*log_2(pj)
			Gini = 1-Σ(pj^2)
		*这两个函数恰好能做到――分类结果(不论正负)越纯洁的叶节点,评价指标越小
		中间节点
			将该节点的每一个叶节点加和
	3.对抗过拟合(剪枝)
		1)限制树的深度(这一条通常也是决定使用哪种 属性选择度量 的标准)max_depth
		2)最少可以分的数量min_samples_split
		3)每个叶子至少含有的样本数
		4)信息增益阈值(通常也作为选择下一个作为分类节点的属性的优化方式)
	4.python实现
		DTree = DecisionTreeClassifier(max_depth,criterion,min_samples_split)
		#如何确定参数值是体现分析师水平的地方
		#对数据进行预处理,将连续性数据变为离散型数据也需要对业务和数据十分熟悉
		#数据清洗、建模、评估、改进优化
		 模型+代码+业务知识
		DTree.fit(X,y)
		DTree.predict(x_test)