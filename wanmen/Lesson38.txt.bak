第三十八次课	卷积神经网络
	1.卷积
		用m1xm2的矩阵扫描n1xn1的矩阵
		每扫描一个区域就进行一次元素乘法并求和
		生成一个新的矩阵
		*对于多输入(如RGB),每一个输入都有一个卷积核进行卷积
		 把所有卷积核的卷积结果相加即为该层的结果
	2.卷积核
		尺寸F:卷积核大小 m1xm2
		深度K:卷积核个数 
		步长S:卷积核每次移动的步数,不一定每次移动一个像素
		零填充P(padding):控制输出图片的大小
	  =>	input 	W1*H1*D1
	  		output	W2 = (W1+2*P-F)/S+1
	  				H2 = (H1+2*P-F)/S+1
	  				D2 = K
	  	*每个卷积核都能够学到某个特征,但是无法事先知道,只能事后反推
	  	*由于使用同一个卷积核进行扫描,因此也被称为共享权重,减少参数个数
	  	*同样由于共享权重,一个卷积核可以通过扫描在任意位置发现对应的特征
	  	 从而能够有效克服图片平移、缩放、旋转等的影响
	3.padding
		已经讲过了,就是为了不让输入数据被迫降维,保证每个数据都有机会作为中心
	4.pooling(为了降维,将指定大小的区域缩为1个元素,其取值由给定的规则决定)
		*pooling的步长和尺寸一般相等
		max pooling(常用):强化该卷积核所学习的特征
		average pooling
		L2-norm pooling
	5.flatten将nxn->1x(n*n)，即矩阵变向量
	6.卷积之后一般跟若干个全连接,最后使用一个softmax
	  *小尺寸、多核、多层
	7.相关经典网络
		Alex-net
		ZF-net
		VGG16-net多核小尺寸
		Inception-net用1x1代替全连接	
		Residual-net建立反向传播高速路
		Dense-net在任意两层之间都建立高速路,防止网络过深使得浅层网络学习结果被遗忘
		
		
	