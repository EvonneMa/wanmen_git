第四十二次课	LSTM
	1.为了解决梯度消失
	2.遗忘门：f(t) = σ(U(f)*x(t) + W(f)*h(t-1))
		决定c(t-1)需要向下传递多少比例的信息
	  输入门：i(t) = σ(U(i)*x(t) + W(i)*h(t-1))
	  		  g(t) = tanh(U(g)*x(t) + W(g)*h(t-1))
	  	决定本次的输入(以g(t)的形式)需要向下传递多少比例的信息
	  更新控制元：c(t) = c(t-1)*f(t) + g(t)*i(t)
	  输出门：o(t) = σ(U(o)*x(t) + W(o)*h(t-1))
	  		  h(t) = o(t)*tanh(c(t))
	3.变形1：将c(t-1)加入f、i、o的计算当中,相当于提前偷看一下c(t)――peephole(最为经典的版本)
	  变形2：以(1-f(t))代替i(t)
	  变形3：GRU门控循环单元,移除c(t),以z(t)来控制学习/遗忘率
	  		 参数更少,鲁棒性更好
	  遗忘门相对更重要
	4.实战：伪装曹雪芹
	  i.输入为三阶张量(batch_size,max_len,len(dict))