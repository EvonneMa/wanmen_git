第十六次课：自然语言学习
	1.语言模型
		词汇表V
		字符串V*
		a.朴素概率分布――字符串V*中每个元素出现的概率
					    N:训练集句子的数量
					    x1...xn:表示某个句子
					    c(x1...xn):该句子出现的次数
				  	    p(x1...xn)=c(x1...xn)/N:该句子出现的概率
				  	  *由于同一句话很难重复出现很多次,故该方案难以实践
		b.马尔科夫过程(条件概率):
			1)给定一个离散随机变量序列X1,X2...,其中Xi∈V
			2)基于条件概率可以给出一阶马尔科夫链
				  P(X1=x1,X2=x2...)=P(X1=x1)ΠP(Xi=xi|Xi-1=xi-1)
			  相应模型即为2元语言模型P(u|v),使用频数之比即可(c(u,v)/c(v))
			  通过训练集得到所有二元模型的概率(即进行参数估计)
			  *同理可以得到任意n阶的表达式
			3)可变长序列:定义Xn=<S>视为句子结束,则序列长度即为n
						 定义x0=x1=*作为开头以使得形式统一(P=ΠP)
			4)模型概率
				对于任一句子s,可以使用训练好的概率计算s出现的概率
					P(s) = ΠP(ui|ui-1) ―――― 以一阶为例(ui∈s)
		  模型评价――迷惑度
			  给定测试集s1,s2,...
		      计算 P_total = logΠP(si)=Σlog(P(si)) ―――― 极大似然估计
			  perplexity = 2^(-L),其中L=1/M*P_total,M为测试集样本容量
		  *注意几个概念
			  训练/测试样本容量M(无论几元模型,容量都是相同的),这归功于*和<S>
			  词汇表V的容量N(对n元模型,搜索范围是N^n)
		  意外情况处理――测试集中出现了训练集中没有的样本
			  1)线性插值平滑――取q(w|u,v)、q(w|u)和q(w)的线性组合
				  			  并使得验证集当中极大似然概率最大化
			  2)拉普拉斯平滑――q(w|u) = [c(w,u)+1]/[c(w)+N]
		c.隐马尔科夫过程――序列到序列(如词性标注)
			输入:x1,x2,...
			输出:y1,y2,...(labels)
		  学习方法:
		  	判别式模型(CRF条件随机场)
		  		从训练集学习P(y1,y2,...|x1,x2,...)
		  		测试集上输出max P(y|x)
		  	生成式模型(HMM隐马尔科夫模型)
		  		从训练集学习P(y,x)
		  		测试集上输出max P(y|x) = max P(x|y)P(y)/P(x),其中P(x)=1
		  HMM:
		  	根据定义可知,HMM是针对词汇x背后的信息y(比如词性)进行建模――――隐
		  	在二元模型下(依然以*开头,以<S>结尾):
		  		p(X,Y) = Πq(yi|yi-1)Πe(xi|yi)   e:emission q:transition
		  	参数估计(即各个概率的估计)
		  		极大似然估计,但最合适的是EM法(expectation maximization)――nltk包所采用
		  	意外情况处理――测试集中出现了训练集中没有的样本
		  		将低频词统一作为一类
		  	模型预测――由于我们建模的是联合概率分布,因此必然存在解码问题
		  		Viterbi算法(动态规划表),一种由后向前的倒推算法
		  		1)计算π(k,u)=max(π(k-1,w)q(u|w)e(xk|u))
		  			//前k-1层都确定时,第k层取第u个元素的概率π(k,u)最大
		  		2)计算bp(k,u)=argmax(π(k-1,w)q(u|w)e(xk|u))
		  			//π(k,u)最大时,第k-1层的元素是w=bp(k,u)
		  		3)计算yn=argmax(π(n,u)q(<S>|u))
		  			//计算yn最有可能作结尾
		  		4)倒推回去即可
		d.神经网络过程――将单词映射成一个向量,对于n-gram是友好的
		  递归神经网络RNN――输入参数非常少,通过循环可以大量预测输出
	2.代码实战(全都是自己编~~~)
		1)读取文件
		2)进行正则表达式替换,删去非汉字字符,统一标点符号,split
		3)使用jieba进行自动分词,构造词汇表V
		4)自定义generate_ngram进行组合,选择一元模型进行测试
		  使用OrderDict进行排序,并画图展示词频分布情况
		5)使用permutation打乱顺序以随机选取训练集、验证集和测试集
		6)使用ngrams_parameter_estimate生成字典、计算样本量
		7)使用jsgl计算概率
		8)主程序调用计算迷惑度
		见NLP.py